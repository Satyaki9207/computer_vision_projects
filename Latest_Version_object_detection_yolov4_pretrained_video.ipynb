{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF5rGAnOF1Gm"
      },
      "source": [
        "# Object detection using the YOLO V4 pre-trained model\n",
        "\n",
        "*by Georgios K. Ouzounis*\n",
        "\n",
        "In this exercise, we will experiment with object detection in a streaming video using the YOLO V4 pretrained model. This is only a demo that will be performed slowly owing to the virtual environment. For substantially improved performance, compile a .py file with all the relevant code and run it locally "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB9dN1Io78DA"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXzDYD5Jqxdo"
      },
      "source": [
        "# import the relevant libraries\n",
        "import numpy as np\n",
        "#import cv2 # openCV\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "w74w0Sa4Zulq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ms4LYX18B-1"
      },
      "source": [
        "## Get the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1TwKBXhHcXu"
      },
      "source": [
        "# first, create a directory to store the model\n",
        "%mkdir model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cY4WZZF1H1nZ"
      },
      "source": [
        "# enter the directory and download the necessary files \n",
        "%cd model\n",
        "!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v3_optimal/yolov4.weights\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/cfg/yolov4.cfg\n",
        "!wget https://raw.githubusercontent.com/AlexeyAB/darknet/master/data/coco.names\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRoo5k-QdB5K"
      },
      "source": [
        "## Customize the YOLO detector\n",
        "\n",
        "class labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zq2FwRd_lso"
      },
      "source": [
        "class_labels_path = \"/content/model/coco.names\"\n",
        "class_labels = open(class_labels_path).read().strip().split(\"\\n\")\n",
        "class_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJu-efpG4f8X"
      },
      "source": [
        "bounding box color definitions: two options"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80wm8OWX_oxr"
      },
      "source": [
        "# declare the repeating bounding box colors for each class\n",
        "# 1st: create a list colors as an RGB string array\n",
        "# Example: Red, Green, Blue, Yellow, Magneta\n",
        "\n",
        "class_colors = [\"255,0,0\",\"0,255,0\",\"0,0,255\",\"255,255,0\",\"255,0, 255\"]\n",
        "\n",
        "#2nd: split the array on comma-separated strings and for change each string type to integer\n",
        "class_colors = [np.array(every_color.split(\",\")).astype(\"int\") for every_color in class_colors]\n",
        "\n",
        "#3d: convert the array or arrays to a numpy array\n",
        "class_colors = np.array(class_colors)\n",
        "\n",
        "#4th: tile this to get 80 class colors, i.e., as many as the classes  (16rows of 5cols each).\n",
        "# If you want unique colors for each class, you may randomize the color generation\n",
        "# or set them manually\n",
        "\n",
        "class_colors = np.tile(class_colors,(16,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PxNlQ1FO4mOI"
      },
      "source": [
        "or random colors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItyWr4oX4pb8"
      },
      "source": [
        "class_colors = np.random.randint(0, 255, size=(len(class_labels), 3), dtype=\"uint8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0aJYLgc1L-q"
      },
      "source": [
        "Declare remaining parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJlg76c61T1Q"
      },
      "source": [
        "# for the image2blob conversion\n",
        "scalefactor = 1.0/255.0\n",
        "new_size = (416, 416)\n",
        "\n",
        "# for the NMS\n",
        "score_threshold = 0.5\n",
        "nms_threshold = 0.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2"
      ],
      "metadata": {
        "id": "fwI5k-eoZ8Ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHo_MgL40uma"
      },
      "source": [
        "## Load the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBTX_4xYAhZc"
      },
      "source": [
        "# Load the pre-trained model \n",
        "yolo_model = cv2.dnn.readNetFromDarknet('model/yolov4.cfg','model/yolov4.weights')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cew-LCPDJOR"
      },
      "source": [
        "# Read the network layers/components. The YOLO V4 neural network has 379 components.\n",
        "# They consist of convolutional layers (conv), rectifier linear units (relu), etc.:\n",
        "\n",
        "model_layers = yolo_model.getLayerNames()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkBIXXd9E_lK"
      },
      "source": [
        "# Loop through all the network layers to find the output layers\n",
        "output_layers = [model_layers[model_layer - 1] for model_layer in yolo_model.getUnconnectedOutLayers()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXYCReuy5e7Q"
      },
      "source": [
        "## Run the model on the live video feed using NMS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDkah5J7zSWZ"
      },
      "source": [
        "install the following two packages to access video content  from www.youtube.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLJAdFFw0r2h"
      },
      "source": [
        "get any video. We have selected the particular one as it shows views of city life "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nsy0lTrF9LeH"
      },
      "source": [
        "import pafy\n",
        "\n",
        "url = \"https://www.youtube.com/watch?v=_MMpKnfT5oU\"\n",
        "video = pafy.new(url)\n",
        "best = video.getbest(preftype=\"mp4\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection_analysis_with_nms(test_img, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold):\n",
        "\n",
        "\t# get the image dimensions  \n",
        "\timg_height = test_img.shape[0]\n",
        "\timg_width = test_img.shape[1]\n",
        "\n",
        "\tresult = test_img.copy()\n",
        "\n",
        "    # declare the lists for the arguments of interest: classID, bbox info, and detection confidences\n",
        "\tclass_ids_list = []\n",
        "\tboxes_list = []\n",
        "\tconfidences_list = []\n",
        "    # loop over each output layer\n",
        "\t\tfor object_detections_in_single_layer in obj_detections_in_layers:\n",
        "\t\t# loop over the detections in each layer\n",
        "\t\tfor object_detection in object_detections_in_single_layer:\n",
        "    # get the confidence scores of all the objects detected with the bounding box\n",
        "\t\t\t\t\t\t\t\t\tprediction_scores = object_detection[5:]\n",
        "\t\t\t# consider the highest score being associated with the winning class\n",
        "\t\t\t# get the class ID from the index of the highest score\n",
        "\t\t\tpredicted_class_id = np.argmax(prediction_scores)\n",
        "\t\t\t# get the prediction confidence\n",
        "\t\t\tprediction_confidence = prediction_scores[predicted_class_id]\n",
        "\n",
        "\t\t\t# consider object detections with confidence score higher than threshold\n",
        "\t\t\tif prediction_confidence > score_threshold:\n",
        "\t\t\t\t# get the predicted label\n",
        "\t\t\t\tpredicted_class_label = class_labels[predicted_class_id]\n",
        "\t\t\t\t# compute the bounding box cooridnates scaled for the input image\n",
        "\t\t\t\tbounding_box = object_detection[0:4] * np.array([img_width, img_height, img_width, img_height])\n",
        "\t\t\t\t(box_center_x_pt, box_center_y_pt, box_width, box_height) = bounding_box.astype(\"int\")\n",
        "\t\t\t\tstart_x_pt = max(0, int(box_center_x_pt - (box_width / 2)))\n",
        "\t\t\t\tstart_y_pt = max(0, int(box_center_y_pt - (box_height / 2)))\n",
        "\n",
        "\t\t\t\t# update the 3 lists for nms processing\n",
        "\t\t\t\t# - confidence is needed as a float \n",
        "\t\t\t\t# - the bbox info has the openCV Rect format\n",
        "\t\t\t\tclass_ids_list.append(predicted_class_id)\n",
        "\t\t\t\tconfidences_list.append(float(prediction_confidence))\n",
        "\t\t\t\tboxes_list.append([int(start_x_pt), int(start_y_pt), int(box_width), int(box_height)])\n",
        "\n",
        "\t# NMS for a set of overlapping bboxes returns the ID of the one with highest \n",
        "\t# confidence score while suppressing all others (non-maxima)\n",
        "\t# - score_threshold: a threshold used to filter boxes by score \n",
        "\t# - nms_threshold: a threshold used in non maximum suppression. \n",
        "\n",
        "\twinner_ids = cv2.dnn.NMSBoxes(boxes_list, confidences_list, score_threshold, nms_threshold)\n",
        "\n",
        "\t# create a list of winner boxes\n",
        "\twinner_box_list = []\n",
        "\n",
        "\tfor winner_id in winner_ids:\n",
        "\t\tmax_class_id = winner_id\n",
        "\t\tbox = boxes_list[max_class_id]\n",
        "\t\tstart_x_pt = box[0]\n",
        "\t\tstart_y_pt = box[1]\n",
        "\t\tbox_width = box[2]\n",
        "\t\tbox_height = box[3]\n",
        "\t\twinner_box_list.append(box)\n",
        "\n",
        "\t\t#get the predicted class id and label\n",
        "\t\tpredicted_class_id = class_ids_list[max_class_id]\n",
        "\t\tpredicted_class_label = class_labels[predicted_class_id]\n",
        "\t\tprediction_confidence = confidences_list[max_class_id]\n",
        "\n",
        "\t\t#obtain the bounding box end co-oridnates\n",
        "\t\tend_x_pt = start_x_pt + box_width\n",
        "\t\tend_y_pt = start_y_pt + box_height\n",
        "\n",
        "\t\t#get a random mask color from the numpy array of colors\n",
        "\t\tbox_color = class_colors[predicted_class_id]\n",
        "\n",
        "\t\t#convert the color numpy array as a list and apply to text and box\n",
        "\t\tbox_color = [int(c) for c in box_color]\n",
        "\n",
        "\t\t# print the prediction in console\n",
        "\t\tpredicted_class_label = \"{}: {:.2f}%\".format(predicted_class_label, prediction_confidence * 100)\n",
        "\t\tprint(\"predicted object {}\".format(predicted_class_label))\n",
        "\n",
        "\t\t# draw rectangle and text in the image\n",
        "\t\tcv2.rectangle(result, (start_x_pt, start_y_pt), (end_x_pt, end_y_pt), box_color, 1)\n",
        "\t\tcv2.putText(result, predicted_class_label, (start_x_pt, start_y_pt-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, box_color, 1)\n",
        "\n",
        "\treturn result, winner_box_list\n",
        "\n"
      ],
      "metadata": {
        "id": "SIYjKx07XlQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def object_detection_iou(iou_image, detection_box, gt_box):\n",
        "\tstart_pt_x_box_a = detection_box[0]\n",
        "\tstart_pt_y_box_a = detection_box[1]\n",
        "\tend_pt_x_box_a = detection_box[0] + detection_box[2]\n",
        "\tend_pt_y_box_a = detection_box[1] + detection_box[3]\n",
        "\tcv2.rectangle(iou_image, (start_pt_x_box_a, start_pt_y_box_a), (end_pt_x_box_a, end_pt_y_box_a), (0, 255, 0), 2)\n",
        "\tcv2.putText(iou_image, \"predicted bbox\", (start_pt_x_box_a, start_pt_y_box_a-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "\n",
        "\tstart_pt_x_box_b = gt_box[0]\n",
        "\tstart_pt_y_box_b = gt_box[1]\n",
        "\tend_pt_x_box_b = gt_box[0] + gt_box[2]\n",
        "\tend_pt_y_box_b = gt_box[1] + gt_box[3]\n",
        "\tcv2.rectangle(iou_image, (start_pt_x_box_b, start_pt_y_box_b), (end_pt_x_box_b, end_pt_y_box_b), (0, 0, 255), 2)\n",
        "\tcv2.putText(iou_image, \"ground truth bbox\", (start_pt_x_box_b, start_pt_y_box_b-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)\n",
        "\n",
        "\t# determine the (x, y)-coordinates of the intersection rectangle\n",
        "\txA = max(start_pt_x_box_a, start_pt_x_box_b)\n",
        "\tyA = max(start_pt_y_box_a, start_pt_y_box_b)\n",
        "\txB = min(end_pt_x_box_a, end_pt_x_box_b)\n",
        "\tyB = min(end_pt_y_box_a, end_pt_y_box_b)\n",
        "\n",
        "\t# compute the area of intersection rectangle\n",
        "\tintersection_area = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
        "\n",
        "\t# compute the areas of both rectangles  separately\n",
        "\tdetArea = (end_pt_x_box_a - start_pt_x_box_a + 1) * (end_pt_y_box_a - start_pt_y_box_a + 1)\n",
        "\tgtArea = (end_pt_x_box_b - start_pt_x_box_b + 1) * (end_pt_y_box_b - start_pt_y_box_b + 1)\n",
        "\tunionArea = detArea + gtArea - intersection_area\n",
        "\n",
        "\t# compute the intersection over union \n",
        "\tiou_value = intersection_area / float(unionArea)\n",
        "\tcv2.putText(iou_image, \"IoU: {:.4f}\".format(iou_value), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\tprint(\"iou = {:.4f}\".format(iou_value))\n",
        "\n",
        "\t# return the intersection over union value\n",
        "\treturn iou_image, iou_value"
      ],
      "metadata": {
        "id": "92Br44u4XyFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aYL1c2gzfcO"
      },
      "source": [
        "mount your Google Drive and get the following file (customize the path; the file is included in the git repo):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yvt-ljHjz1gF"
      },
      "source": [
        "#%cp /content/drive/MyDrive/object_detection/object_detection_functions.py ."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt1r1WxTz3uX"
      },
      "source": [
        "#from object_detection_functions import object_detection_analysis_with_nms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFu03CtN-iJn"
      },
      "source": [
        "**WARNING:** this will be a very slow loop in part owing to the cv2_imshow() command. Everyframe processed will be displayed after the previous one. To break this loop go to Runtime->Interrupt Execution\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5zjbPZ7DSur"
      },
      "source": [
        "cap = cv2.VideoCapture(best.url)\n",
        "\n",
        "new_width = 640\n",
        "new_height = 480\n",
        "dim = (new_width, new_height)\n",
        "\n",
        "if cap.isOpened():\n",
        "  while True:\n",
        "    #get the current frame from the video stream\n",
        "    ret,frame = cap.read()\n",
        "\n",
        "    frame = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
        "\n",
        "    blob = cv2.dnn.blobFromImage(frame, scalefactor, new_size, swapRB=True, crop=False)\n",
        "\n",
        "    # input preprocessed blob into the model\n",
        "    yolo_model.setInput(blob)\n",
        "\n",
        "    # compute the forward pass for the input, storing the results per output layer in a list\n",
        "    obj_detections_in_layers = yolo_model.forward(output_layers)\n",
        "\n",
        "    # get  the object detections drawn on  the frame\n",
        "    frame, winner_boxes = object_detection_analysis_with_nms(frame, class_labels, class_colors, obj_detections_in_layers, score_threshold, nms_threshold)\n",
        "\n",
        "    #display the frame\n",
        "    cv2_imshow(frame)\n",
        "    # if running outside Colab notebooks use:\n",
        "    # cv2.imshow(frame)\n",
        "\n",
        "    #terminate while loop if the 'q' key is pressed - applicable outside the notebooks\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "      break\n",
        "\n",
        "  #releasing the stream and the camera\n",
        "  cap.release()\n",
        "  cv2.destroyAllWindows()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9n-tCxBphgju"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}